{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "711c0b74",
   "metadata": {},
   "source": [
    "### Assignment 3\n",
    "\n",
    "These supplementary questions are meant to substitute for assignment 3. If applicable, you are welcome to attach an image from a drawing or provide examples to illustrate your answers. You are not required to include code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d0ee3",
   "metadata": {},
   "source": [
    "#### Part 1\n",
    "* The following questions are related to the notebook `topic-modeling-with-bert.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1462af62",
   "metadata": {},
   "source": [
    "1. Knowing that HDBSCAN has a complexity of $n^2$, what are the big-data-related bottlenecks of the above analysis? \n",
    "\n",
    "  * Describe in detail how you'd handle each bottleneck.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ade6ca-7200-434d-944d-0240611c3f78",
   "metadata": {},
   "source": [
    "### Answer 1:\n",
    "\n",
    "Given the size/volume of big data, a quadratic time complexity for an algorithm means that on a large data set the data will take an unreasonable amount of time to process. A solution to this issue is dimensionality reduction. Reducing the dimensions of the embedding so a manageble size for HDBSCAN will allow the algorithm to run in a reasonable amount of time. In the notebook above, UMAP dimensionalty reduction is first applied to the data, the lower dimensional data is the passed into HDBSCAN for clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c6bf1d",
   "metadata": {},
   "source": [
    "2. What improvements would you make to the analysis above? From an analytical perspective rather than a computational one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7fb105-21be-424d-b095-8ab6b26d97db",
   "metadata": {},
   "source": [
    "### Answer 2:\n",
    "\n",
    "The additional step of pre-processing the data more thoroughly might help with analysis. In the topics we see stop words and duplicates. Removing those in the beginning would give better insight into the content of the documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4eeea7",
   "metadata": {},
   "source": [
    "3. How would you handle instances assigned a -1 by HDBSCAN? I.e., 1. would you throw them away or would you assingn them back into the cluster by using \"some approach or heuristitic\" to do so?  If you chose 1, then describe why? If you chose 2., describe how?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5833025c-bb94-426a-b3b3-0785a1c8e672",
   "metadata": {},
   "source": [
    "### Answer 3:\n",
    "\n",
    "One approach would be to adjust the amount of data going in to HDBSCAN. It's possible that in the dimensionality reduction key features were lost that would have provided a more accurate classification for that data. So increasing the dimensionality and re-clustering is one solition. Additionally, tweaking the `cluster_selection_method` parameter and changing the setting to `leaf` could reduce the -1 instances. According to the documentation `leaf` produces more fine grained homogenous clusters which could be able to accurately group the -1 instances. Another parameter the could be tweaked in the `min_cluster_size`. By reducing the minimum for clusters more of the -1 data points would likely find a home."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f765c8ae",
   "metadata": {},
   "source": [
    "\n",
    "4. K-means would work very well with the cosine distance. How would you go about findind the best value of k?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c6b7c-f540-499e-aad9-59b17f4b5e47",
   "metadata": {},
   "source": [
    "### Answer 4:\n",
    "\n",
    "K-means works best with apriori knowledge of the data. In the case where there is no knowledge of the data or an expected number of clusters for k, one solution could be to run HDBSCAN and see what clusters are produced, and select that number for k. Another method would be experimenting with different values of K and then check the silhoette score for k to determine the best choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52cc953",
   "metadata": {},
   "source": [
    "5. Discribe the approch you'd use to reduce the number of topics obtained?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603650c7-11ce-425b-8d63-a0f250c17dc1",
   "metadata": {},
   "source": [
    "### Answer 5:\n",
    "\n",
    "To reduce the number of topics obtained I would merge similar topics by establishing some threshold for minimum cosine distance. So points in the cluster would need to be at lease the min_cos_distance apart, to be considered different topics, otherwise they'd be merged. That way a topic like 'bike' and 'bikes', which are two seperate topics in the notebook, would be considered the same topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b6e1e",
   "metadata": {},
   "source": [
    "#### Part 2\n",
    "\n",
    "* The following questions are related to the notebook `sentiment_mining`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d0c80b",
   "metadata": {},
   "source": [
    "Satisfied with your analysis, you decide to scale it for your company to run on massive amounts data daily. To do so, you decide to use the Amazon cloud. Your job here is to explore the various web services offered by amazon, identify those that are most relevant to each step in the analysis and descrive how and why you chose to include it in your pipeline. Your company decide that uploading the data to Pinecone daily is costly (Amazon charges by GB of data transferred), so you will also need to identify a service that can do something similar to what you did with Pinecone.\n",
    "\n",
    "\n",
    "Note that Amazon does not have a vector (or embedding DB), so you need to find a service within the Amazon ecosystem that provides nearest neighbor search.\n",
    "\n",
    "Provide an overview diagram that describes the services you will be using and how the data will flow between each service.\n",
    "\n",
    "For each service in your diagram, describe which task this service is doing and why you chose it. \n",
    "\n",
    "You can find a list of AWS products [here.](https://aws.amazon.com/products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab5b70e-dbbb-45d6-adfe-ba04e66eb87f",
   "metadata": {},
   "source": [
    "### Diagram\n",
    "\n",
    "![AWS_Diagram](CE_AWS_Diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5fc9c1-5365-46ae-9d75-7e392240d36d",
   "metadata": {},
   "source": [
    "<b> Amazon Kinesis </b> : A cloud based services that allows streaming of large amounts of data per second from multiple data sources.In the diagram Kinesis is used to ingest data from multiple sources and pass it into S3 for storage.\n",
    "\n",
    "<b> Amazon S3 </b>: S3 is Amazons data lake, in the first instance we use the S3 as storage for streaming unstructured data, in the second instance we use it to house some of cleaned data from Amazon EMR\n",
    "\n",
    "<b> Amazon EMR </b>: Amazon EMR is used to provide distributed data processing using tools like Apache Spark or Hadoop. The processed data is then sent into S3 or Redshift.\n",
    "\n",
    "<b> Amazon Redshift </b>: Redshift is Amazon's data warehouse. In this diagram it is used to store strutured data for BI and Analytics tasks \n",
    "\n",
    "<b> Amazon DynamoDB </b>: Is used to hold metadata and vectors for our object stored in S3; the data in DynamoDB would be used my Data Science end users and ML/AI tasks\n",
    "\n",
    "<b> Amazon Athena </b>: Athena can be used by the end user to preform analysis on both structured and unstructered data\n",
    "\n",
    "<b> Amazon QuickSight </b>: Quicksight can be used for business intelligence analysis; it is comparable to Tableau"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
