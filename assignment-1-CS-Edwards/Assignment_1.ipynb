{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1443cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Candace Edwards Save-Test-Push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa8d15e",
   "metadata": {},
   "source": [
    "# Assignment 1: Data Analysis with Spark.\n",
    "\n",
    "\n",
    "For this assignment, you will need to make sure you're running from a PySpark Docker environment I introduced in class. You can start the pySpark Docker environment using the following command:\n",
    "\n",
    "```\n",
    "docker run --rm -p 4040:4040 -p 8888:8888 -v $(pwd):/home/jovyan/work jupyter/all-spark-notebook\n",
    "```\n",
    "\n",
    "Make sure you run the command from the directory containing this Jupyter notebook and your data folder.\n",
    "\n",
    "> ⚠️ For some reason, my Jupyter Notebook didn't always sync properly when I was pushing to github. As such, please push often and make sure your incremental changes appear on GitHub.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b0d3e6",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "The first part will use Spark to analyze the following books, which I have downloaded for you to use from Project Gutenberg. The files are saved to the data folder.\n",
    "\n",
    "| File name | Book Title|\n",
    "|:---------:|:----------|\n",
    "|43.txt | The Strange Case of Dr. Jekyll and Mr. Hyde by Robert Louis Stevenson|\n",
    "|84.txt | Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley |\n",
    "|398.txt  | The First Book of Adam and Eve by Rutherford Hayes Platt|\n",
    "|3296.txt | The Confessions of St. Augustine by Bishop of Hippo Saint Augustine|\n",
    "\n",
    "The objective is to explore whether we can detect similarity between books within the same topic using word-based similarity. \n",
    "\n",
    "The task of identifying similar texts in Natural Language Processing is crucial. A naive method for determining whether two documents are similar is to treat them as collections of words (bag of words) and use the number of words they share as a proxy for their similarity. It makes sense that two books with religion as the topic (e.g., `398.txt` and `3296.txt`) would have more words in common than a book that discusses religion and a book that discusses science fiction (e.g. books `84.txt` and `398.txt`). \n",
    "\n",
    "As mentioned above, we will be using Spark to analyze the data. Although Spark is not needed for such a small example, the platform would be ideal for analyzing very large collections of documents, like those often analyzed by large corporations\n",
    "\n",
    "This part of the assignment will rely exclusively on RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a15530",
   "metadata": {},
   "source": [
    "### Q1. \n",
    "Start by importing Spark and making sure your environment is set up properly for the assignment.\n",
    "\n",
    "Import the Spark context necessary to load a document as an RDD; ignore any error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e80df4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "#SparkContext?\n",
    "sc=SparkContext()\n",
    "#sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336d5c9d",
   "metadata": {},
   "source": [
    "### Q2 \n",
    "\n",
    "Read in the file `43.txt` as a Spark RDD and save it to a variable called `book_43`\n",
    " * make sure `book_43` is of type MapPartitionsRDD, i.e.,\n",
    "   * str(type(book_43)) == \"<class 'pyspark.rdd.RDD'>\" should return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d4164f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_43 = sc.textFile('data/43.txt')\n",
    "str(type(book_43)) == \"<class 'pyspark.rdd.RDD'>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8c051e",
   "metadata": {},
   "source": [
    "### Q3\n",
    "\n",
    "How many lines does `book_43` contain?\n",
    "* You can only use operations or actions on RDDs to answer the question. \n",
    "  * Code that uses methods such as `some_rdd.X().Y().Z()...` is allowed\n",
    "  * Code that uses functions such as `some_func(...)` is not allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3c5a06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_43 contains 2935 lines\n"
     ]
    }
   ],
   "source": [
    "book_43_count = book_43.count()\n",
    "print(f\"book_43 contains {book_43_count} lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca2bc6d",
   "metadata": {},
   "source": [
    "### Q4 \n",
    "\n",
    "Prior to analyzing the words contained in this book, we need to first remove the occurrences of non-alphabetical characters and numbers from the text. You can use the following function, which takes a line as input, removes digits and non-word characters, and splits it into a collection of words. \n",
    "\n",
    "```python\n",
    "def clean_split_line(line):\n",
    "    a = re.sub('\\d+', '', line)\n",
    "    b = re.sub('[\\W]+', ' ', a)\n",
    "    return b.upper().split()\n",
    "```\n",
    "\n",
    "Use the function above on the variable (test_line) to see what it returns.\n",
    "```python\n",
    "test_line = \"This is an example of that contains 234 and a dash-containing number\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55ce2754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'an', 'example', 'of', 'that', 'contains', 'and', 'a', 'dash', 'containing', 'number']\n"
     ]
    }
   ],
   "source": [
    "def clean_split_line(line):\n",
    "    a = re.sub('\\d+', '', line)\n",
    "    b = re.sub('[\\W]+', ' ', a)\n",
    "    return b.lower().split()\n",
    "\n",
    "test_line = \"This is an example of that contains 234 and a dash-containing number\"\n",
    "\n",
    "\n",
    "result = clean_split_line(test_line)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e951e5",
   "metadata": {},
   "source": [
    "### Q5\n",
    "\n",
    "How many words does `book_43` contain? To answer this question, you may find it useful to apply the function in a Spark-like manner. \n",
    "* You can only use operations or actions on RDDs to answer the question. \n",
    "  * Code that uses methods such as `some_rdd.X().Y().Z()...` is allowed\n",
    "  * Code that uses functions such as `some_func(...)` is not allowed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99c1b93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_43 contains 29116 words\n"
     ]
    }
   ],
   "source": [
    "RDD = book_43.flatMap(clean_split_line)\n",
    "print(f\"book_43 contains {RDD.count()} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d62952",
   "metadata": {},
   "source": [
    "### Q6\n",
    "\n",
    "How many of the words in book_43 are unique? Given that words can appear in lower, upper or mixed case (ex. The, THE, the), make sure you convert the words into lower case before counting them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07785c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_43 contains 4296 UNIQUE words\n"
     ]
    }
   ],
   "source": [
    "distinct_words = RDD.distinct().count()\n",
    "print(f\"book_43 contains {distinct_words} UNIQUE words\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b13b79",
   "metadata": {},
   "source": [
    "### Q7\n",
    "\n",
    "* Generate an `RDD` that contains the frequency of each word in `book_43`. Call the variable `book_43_counts`. Each item in the `RDD` should be a tuple with the word as the first element of the tuple and the count as the second item of the tuple. The collection should look like the following:\n",
    "\n",
    "[('project', 88), (\"the\", 1807), ... ]\n",
    "\n",
    "* Such a collection may contain a large number of words and it would be imprudent to transfer all the words onto the same machine to show them. Instead, to explore the content of such a collection, display only the first element in your list. \n",
    "\n",
    "* Given the random nature of this operation, the first element element displayed may be different. The first entry for me was:\n",
    "```\n",
    "[('project', 88)]\n",
    "```\n",
    "\n",
    "* You can only use operations or actions to answer the question. \n",
    "* Code that uses methods such as `some_rdd.X().Y().Z()...` is allowed\n",
    "* Code that uses functions such as `some_func(...)` is not allowed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42729b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zip', 1),\n",
       " ('youth', 2),\n",
       " ('yourself', 6),\n",
       " ('yours', 2),\n",
       " ('your', 64),\n",
       " ('younger', 3),\n",
       " ('young', 5),\n",
       " ('you', 312),\n",
       " ('yet', 32),\n",
       " ('yesterday', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_mapped = RDD.map(lambda x:(x,1))\n",
    "rdd_grouped = rdd_mapped.groupByKey()\n",
    "book_43_counts = rdd_grouped.mapValues(sum).map(lambda x:(x[0],x[1])).sortByKey(False)\n",
    "book_43_counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee4534c",
   "metadata": {},
   "source": [
    "### Q8\n",
    "\n",
    "Sort `book_43_counts` and print the 20 most frequent words in book_43. \n",
    "  * Hint: the function `sortByKey` sorts a collection of tuples on the first element element of the list. You can easily change the order of the items in each element and use `sortByKey` to sort on the second item of each element in `book_43_counts`\n",
    "  * You can only use operations or actions to answer the question. \n",
    "  * Code tha employs methods such as `some_rdd.X().Y().Z()...` is allowed\n",
    "  * Code that uses functions such as `some_func(...)` is not allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d51b8ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1807, 'the'),\n",
       " (1068, 'of'),\n",
       " (1043, 'and'),\n",
       " (726, 'to'),\n",
       " (686, 'a'),\n",
       " (646, 'i'),\n",
       " (485, 'in'),\n",
       " (471, 'was'),\n",
       " (392, 'that'),\n",
       " (384, 'he'),\n",
       " (378, 'it'),\n",
       " (312, 'you'),\n",
       " (308, 'my'),\n",
       " (301, 'with'),\n",
       " (285, 'his'),\n",
       " (244, 'had'),\n",
       " (203, 'as'),\n",
       " (202, 'for'),\n",
       " (195, 'this'),\n",
       " (193, 'but')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_rdd_frequency = rdd_grouped.mapValues(sum).map(lambda x:(x[1],x[0])).sortByKey(False)\n",
    "sorted_rdd_frequency.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13420b09",
   "metadata": {},
   "source": [
    "### Q9\n",
    "\n",
    "You must have noted that the most frequent words in `book_43_counts` include stop words such as `of`, `the`, `and`, etc.\n",
    "\n",
    "It would be inefficient to compare documents based on whether or not they contain stop words; those are common to all documents. As such, it's common to remove such stop words. The library `sklearn.feature_extraction` provides access to a collection of English stop words, which can be loaded using the following snippet:\n",
    "\n",
    "```\n",
    "from sklearn.feature_extraction import stop_words\n",
    "stop_words.ENGLISH_STOP_WORDS\n",
    "```\n",
    "\n",
    "* Explore ENGLISH_STOP_WORDS (it's a frozen set data structure, i.e., a set that you cannot modify) by printing any 10 words from it. \n",
    " * Hint: convert the frozen set to something you can subscript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e0ad996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seemed', 'along', 'please', 'either', 'after', 'below', 'must', 'whom', 'onto', 'ltd']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import _stop_words\n",
    "stop = list(_stop_words.ENGLISH_STOP_WORDS)\n",
    "print(stop[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625eb521",
   "metadata": {},
   "source": [
    "### Q10\n",
    "\n",
    "Filter out the words in `book_43_counts` by removing those that appear in the ENGLISH_STOP_WORDS.\n",
    "Save the results to a new variable called `book_43_counts_filtered`\n",
    "  * You can only use operations or actions on RDDs to answer the question. \n",
    "  * Code that uses methods such as `some_rdd.X().Y().Z()...` is allowed\n",
    "  * Code that uses functions such as `some_func(...)` is not allowed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5602b4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(156, 's'),\n",
       " (131, 'utterson'),\n",
       " (130, 'said'),\n",
       " (128, 'mr'),\n",
       " (105, 'jekyll'),\n",
       " (105, 'hyde'),\n",
       " (98, 'gutenberg'),\n",
       " (88, 'project'),\n",
       " (85, 'man'),\n",
       " (72, 'lawyer')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_43_counts_filtered=book_43_counts.filter(lambda x:x[0] not in stop)\n",
    "book_43_counts_f_sort = book_43_counts_filtered.map(lambda x:(x[1],x[0])).sortByKey(False)\n",
    "book_43_counts_f_sort.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0b3a80",
   "metadata": {},
   "source": [
    "### Q11\n",
    "\n",
    "*How many words remain in `book_43_counts_filtered` after removing the stop words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5bdf083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4034"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_43_counts_filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ffd755",
   "metadata": {},
   "source": [
    "### Q12 \n",
    "\n",
    "* Create a function called *process_RDD* that combines the relevant steps you proposed above to make it convenient to apply them to the remaining four books. Your function should accept an input text file path and:\n",
    " * Reads in the file as a textRDD\n",
    " * Cleans and splits the line using `clean_split_line`\n",
    " * Filters out the stop words\n",
    " * Returns a word count RDD where each item is a tuple of words and its count.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eff5d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_RDD(path):\n",
    "    RDD = sc.textFile(path)\n",
    "    RDD_word_count = RDD.flatMap(clean_split_line).filter(lambda x:x not in stop).map(lambda x:(x,1)).groupByKey().mapValues(sum).map(lambda x:(x[0],x[1])).sortByKey(False)\n",
    "    return RDD_word_count\n",
    "\n",
    "#testRDD = process_RDD('data/43.txt')\n",
    "#testRDD.count()\n",
    "#testRDD.take(5)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba59c9c5",
   "metadata": {},
   "source": [
    "### Q13 \n",
    "\n",
    "Apply the function `process_RDD` to `book_84`, `book_398` and `book_3296` and save the results to variables `book_84_counts_filtered`, `book_398_counts_filtered` and `book_3296_counts_filtered` respectively. \n",
    "\n",
    "How many distinct words does each book contain after filtering the stop words?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08a24f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book 84 contains 7016 unique words\n",
      "book 398 contains 2421 unique words\n",
      "book 3296 contains 7293 unique words\n"
     ]
    }
   ],
   "source": [
    "book_84_counts_filtered = process_RDD('data/84.txt')\n",
    "book_398_counts_filtered = process_RDD('data/398.txt')\n",
    "book_3296_counts_filtered = process_RDD('data/3296.txt')\n",
    "\n",
    "print(f\"book 84 contains {book_84_counts_filtered.distinct().count()} unique words\")\n",
    "print(f\"book 398 contains {book_398_counts_filtered.distinct().count()} unique words\")\n",
    "print(f\"book 3296 contains {book_3296_counts_filtered.distinct().count()} unique words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87df26f6-eebb-4ad2-abfc-732815bcdef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#book_84_counts_filtered.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb9680f",
   "metadata": {},
   "source": [
    "### Q14 \n",
    "\n",
    "The similarity between two texts can be assessed using the number of words the two tests share. For example, there are probably more words in common between two books that discuss Greek history than they would share with books on computing. Let's hypothesize that books with similar themes will have more words in common than books with different themes. Assuming this hypothesis is correct, book_398 and book_3296, two books that both deal with religion, will have more words in common than, for example, book_84 and book_398.\n",
    "\n",
    "Test this hypothesis by writing code that compares and prints the number of words shared between `book_398` and `book_3296` and then between `book_84` and `book_398`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10a03d32-3fb3-4ce4-bdfc-f2f2be7b837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION TEST CODE#\n",
    "#RDD_1_test =sc.parallelize([('Candace',1000000),('Lee',1000000),('Lydia',2000000),('Lydia',3000000)])\n",
    "#RDD_2_test =sc.parallelize([('Candace',1000000),('Jane',1000000),('Lydia',1000000),('Lee',1000000)])\n",
    "\n",
    "#RDD_3= sc.parallelize(['Racks getting larger just look at the alkaline wrist cause I got that water test test'])\n",
    "#RDD_4= sc.parallelize(['Racks getting thicker just look at the acidic wrist cause I got that fire test test'])\n",
    "\n",
    "\n",
    "#print(f'rdd3 and rdd4 have {compareRDD(RDD_3,RDD_4)} words in common')\n",
    "\n",
    "##pseudo\n",
    "## create RDDs with distinct values\n",
    "## get intersection of RDDs\n",
    "\n",
    "#new_rdd_3 = RDD_3.flatMap(clean_split_line).distinct().filter(lambda x: x not in stop)\n",
    "#new_rdd_4 = RDD_4.flatMap(clean_split_line).distinct().filter(lambda x: x not in stop)\n",
    "#rdd_comparison = new_rdd_3.intersection(new_rdd_4)\n",
    "#rdd_comparison.count() #expected 8, observed 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac6b73e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareRDD(RDD_1,RDD_2):\n",
    "    new_rdd_1 = RDD_1.flatMap(clean_split_line).distinct().filter(lambda x: x not in stop)\n",
    "    new_rdd_2 = RDD_2.flatMap(clean_split_line).distinct().filter(lambda x: x not in stop)\n",
    "    rdd_comparison = new_rdd_1.intersection(new_rdd_2)\n",
    "    return rdd_comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ffb9d77-7ede-47e1-b0cb-1f2a9a4511aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_398and book_3296 have 1790 words in common\n",
      "book_398 and book_84 have 1691 words in common\n"
     ]
    }
   ],
   "source": [
    "book_398 = sc.textFile('data/398.txt')\n",
    "book_3296 = sc.textFile('data/3296.txt')\n",
    "book_84 = sc.textFile('data/84.txt')\n",
    "\n",
    "compare_1=compareRDD(book_398,book_3296)\n",
    "compare_2=compareRDD(book_398,book_84)\n",
    "\n",
    "print(f'book_398and book_3296 have {compare_1.count()} words in common')\n",
    "print(f'book_398 and book_84 have {compare_2.count()} words in common')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef9f326",
   "metadata": {},
   "source": [
    "### Q15\n",
    "\n",
    "* Based on the above, do you think counting the number of shared words is a good idea as a distance metric for evaluating topic similarity? Justify your answer?\n",
    "* Hint: What do *book_84* and *book_3296* have in common? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2fd43a2-979d-436f-88af-c302ecc2ed94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3608"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_3=compareRDD(book_84,book_3296)\n",
    "compare_3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6587ae14",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "Comparing common words in text is not a good indicator of similarity in the overall text. While the religioun themed books (book_398 and book_3296) have more words in common, it was only a ~10% difference compared to the words in common of the books with seperate themes (books_398 and book_84). Additionally when comparing book_84 and book_3296, these two books with differing themes had the most words in common of all the previous comparisons. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f8f857",
   "metadata": {},
   "source": [
    "## Part II \n",
    "\n",
    "Another approach to estimating similarity consists of computing the Euclidean distance across a set of words. For example, suppose we have 3 documents A, B and C with the following counts for the words `evolution`, `DNA`, `biology` and `finance`. \n",
    "\n",
    "```python \n",
    "A = [4, 9, 6, 8]\n",
    "B = [3, 7, 7, 10]\n",
    "C = [15, 10, 1, 1]\n",
    "```\n",
    "Although all documents contain exactly the four words, the number of times these words appear in each book may be indicative of their topic. For example, documents `A` and `B` are more likely to be business related since they contain the word `finance` more frequently (8 and 10 times respectively). Document `C` may be a technical document since it focuses on more technical words (`evolution` and `DNA`) and less on the words `finance`.\n",
    "\n",
    "The Euclidean distance can be computed using the `scikit` snippet below:\n",
    "\n",
    "```python\n",
    "from scipy.spatial.distance import euclidean \n",
    "print(f\"The Euclidean distance between A and B is: {euclidean(A, B)}\")\n",
    "\n",
    "print(f\"The Euclidean distance between A and C is: {euclidean(A, C)}\")\n",
    "\n",
    "print(f\"The Euclidean distance between B and C is: {euclidean(B, C)}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abdc35e",
   "metadata": {},
   "source": [
    "### Q16\n",
    "\n",
    "In order to calculate the Euclidean distance, we must first identify the set of words on which we will compare the documents. First, we will use the words that appear to all 4 documents. We will store the data in a matrix called `counts_matrix` where the books will be represented as rows and the words will be represented as columnns. \n",
    "\n",
    "Start by finding the words that are common to all four documents after stop-word filtering and store the counts for each word in a column of `counts_matrix`. \n",
    "\n",
    "Based on the previous example, the following code will create an emtpy matrix with three lines (documents A, B, and C) and four columns (words evolution, DNA, biology, and finance).\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "counts_matrix = np.zeros([3,4])\n",
    "```\n",
    "\n",
    "After generating the counts, you can fill the counts for a document, say `A`, using the following code:\n",
    "\n",
    "```python\n",
    "counts_matrix[0, :] = [4, 9, 6, 8] \n",
    "```\n",
    "* Other than for building `counts_matrix` you should exclusively use operations or actions on the `RDD` to answer this question. \n",
    "  * Code that uses methods such as `some_rdd.X().Y().Z()...` is allowed\n",
    "  * Code that uses functions such as `some_func(...)` is not allowed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3cae946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  3.  5. ...  2.  3.  1.]\n",
      " [ 1.  9. 30. ...  4.  9.  1.]\n",
      " [ 1.  2.  1. ... 16.  5.  1.]\n",
      " [ 1.  4. 15. ...  3. 36. 16.]]\n",
      "count_matrix has 4 rows and 1162 columns\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "#book_43_counts_filtered\n",
    "#book_84_counts_filtered = process_RDD('data/84.txt')\n",
    "#book_398_counts_filtered = process_RDD('data/398.txt')\n",
    "#book_3296_counts_filtered = process_RDD('data/3296.txt')\n",
    "\n",
    "#common words among documents\n",
    "rdd_join_col = book_43_counts_filtered.join(book_84_counts_filtered).join(book_398_counts_filtered).join(book_3296_counts_filtered).map(lambda x:x[0]) #join and map create list of column words\n",
    "#rdd_join_col.take(10)\n",
    "words = rdd_join_col.collect()\n",
    "\n",
    "m = 4 #books/rdds/rows\n",
    "n = rdd_join_col.count() #columns count\n",
    "\n",
    "rdd_b0 = book_43_counts_filtered.filter(lambda x: x[0] in words).map(lambda x:x[1])\n",
    "rdd_b1 = book_84_counts_filtered.filter(lambda x: x[0] in words).map(lambda x:x[1]) # filter by key in list of col_words create value list\n",
    "rdd_b2 = book_398_counts_filtered.filter(lambda x: x[0] in words).map(lambda x:x[1]) # filter by key in list of col_words create value list\n",
    "rdd_b3 = book_3296_counts_filtered.filter(lambda x: x[0] in words).map(lambda x:x[1]) # filter by key in list of col_words create value list\n",
    "\n",
    "counts_matrix = np.zeros([m,n])\n",
    "\n",
    "\n",
    "counts_matrix[0,:] = rdd_b0.collect()\n",
    "counts_matrix[1,:] = rdd_b1.collect()\n",
    "counts_matrix[2,:] = rdd_b2.collect()\n",
    "counts_matrix[3,:] = rdd_b3.collect()\n",
    "\n",
    "#print(f'documents have {rdd_join_col.count()} words in common')\n",
    "print(counts_matrix[:][:10])\n",
    "print(f'count_matrix has {m} rows and {n} columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7a7d561-f82d-4c03-85de-fa6e91b4ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTCODE ##SANDBOX\n",
    "\n",
    "#A = sc.parallelize([4, 9, 6, 8])\n",
    "#B = sc.parallelize([3, 7, 7, 10])\n",
    "#C = sc.parallelize([15, 10, 1, 1])\n",
    "\n",
    "#import numpy as np\n",
    "#counts_matrix = np.zeros([3,4])\n",
    "#counts_matrix[0,:] = A.collect()\n",
    "#counts_matrix[1,:] = B.collect()\n",
    "#counts_matrix[2,:] = C.collect()\n",
    "#print(counts_matrix[0][0])\n",
    "\n",
    "#RDD_t1=sc.parallelize([('word',10),('bell',11),('pillow',12),('lamp',13)])\n",
    "#RDD_t2=sc.parallelize([('word',20),('bell',21),('pillow',22),('lamp',23),('misc',15)])\n",
    "#RDD_t3=sc.parallelize([('word',30),('bell',31),('pillow',32),('lamp',33),('test',15)])\n",
    "#RDD_t4=sc.parallelize([('word',40),('bell',41),('lamp',43)])\n",
    "\n",
    "#rdd_join_two = RDD_t1.join(RDD_t2)\n",
    "#rdd_join_col = rdd_join_two.join(RDD_t3).join(RDD_t4).map(lambda x:x[0]) #join and map create list of column words\n",
    "#n = rdd_join_col.count() #columns count\n",
    "#m = 4 #books/rdds\n",
    "#words = rdd_join_col.collect()\n",
    "#rdd_join_count_t1 = RDD_t1.filter(lambda x: x[0] in words).map(lambda x:x[1]) # filter by key in list of col_words create value list\n",
    "#rdd_join_count_t2 = RDD_t2.filter(lambda x: x[0] in words).map(lambda x:x[1]) # filter by key in list of col_words create value list\n",
    "#rdd_join_count_t3 = RDD_t3.filter(lambda x: x[0] in words).map(lambda x:x[1]) # filter by key in list of col_words create value list\n",
    "#rdd_join_count_t4 = RDD_t4.filter(lambda x: x[0] in words).map(lambda x:x[1]) # filter by key in list of col_words create value list\n",
    "\n",
    "#pseudo:\n",
    "#on given RDD, key is in list of common words,map to value\n",
    "\n",
    "#rdd_join_count_t1.take(10)\n",
    "\n",
    "\n",
    "\n",
    "##counts_matrix = np.zeros([m,n])\n",
    "\n",
    "#counts_matrix[0,:] = rdd_join_count_t1.collect()\n",
    "#counts_matrix[1,:] = rdd_join_count_t2.collect()\n",
    "#counts_matrix[2,:] = rdd_join_count_t3.collect()\n",
    "#counts_matrix[3,:] = rdd_join_count_t4.collect()\n",
    "\n",
    "#counts_matrix[1,:] = B.collect()\n",
    "#counts_matrix[2,:] = C.collect()\n",
    "\n",
    "\n",
    "\n",
    "#print(counts_matrix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#disgarded code#\n",
    "#rdd_join_count = rdd_join_two.join(RDD_t3).join(RDD_t4).map(lambda x:x[1]) #join and map create list of counts-NO\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124bcdd8",
   "metadata": {},
   "source": [
    "### Q17\n",
    "\n",
    "Compute the Euclidean distance between `book_398` and `book_3296`, which both talk about religion and `book_84` and `book_398`. \n",
    "What do you conclude about using the Euclidean distance for evaluating topic relatedness across documents?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b6a81dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Euclidean distance between book_398 and book_3296 is: 1156.6628722320086\n",
      "The Euclidean distance between book_398 and book_84 is: 751.6688100486809\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import euclidean \n",
    "print(f\"The Euclidean distance between book_398 and book_3296 is: {euclidean(counts_matrix[2,:], counts_matrix[3,:])}\")\n",
    "print(f\"The Euclidean distance between book_398 and book_84 is: {euclidean(counts_matrix[2,:], counts_matrix[1,:])}\")\n",
    "#print(f\"The Euclidean distance between B and C is: {euclidean(B, C)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89981b46",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "\n",
    "Euclidean distance is a sub-optimal technique for measuring similarity between documents. In the results above,the similarly themed book_398 and book_3296 are more distant than book_84 and book_398.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e54e6",
   "metadata": {},
   "source": [
    "### Q18\n",
    "\n",
    "Bonus question (5 points): Can you think of a few things we could do to improve similarity between documents that pertain to the same topic. Justify your answer without giving code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8c6de1",
   "metadata": {},
   "source": [
    "#### Write your answer here\n",
    "\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf1d74",
   "metadata": {},
   "source": [
    "## Part III\n",
    "\n",
    "As part of this section, we will create some basic analytics for a dataset containing flight arrivals and departures within the United States over the course of a month. \n",
    "Pandas can be used to manage this dataset (1M records), but a distributed computing framework like Spark, will be beneficial when extending the analysis to a longer timeframe.\n",
    "\n",
    "Here, you should use exclusively `SparkDatFrames. \n",
    "\n",
    "This dataset can be analyzed to improve trip scheduling.  For example:\n",
    " * Avoid airlines carriers that are most often associated with delays.\n",
    " * Avoid departure days where delays are most frequent.\n",
    " * Avoid airports which are associated with delays or long taxxying time.\n",
    "* etc.\n",
    " \n",
    "\n",
    "\n",
    "The information about the fields contained in the data file can be found [here](https://dataverse.harvard.edu/dataset.xhtml;jsessionid=0414e25969eccd0e88ae4d64fa0b?persistentId=doi%3A10.7910%2FDVN%2FHG7NV7&version=&q=&fileTypeGroupFacet=&fileTag=%221.+Documentation%22&fileSortField=date&fileSortOrder=desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149805af",
   "metadata": {},
   "source": [
    "### Q19\n",
    "\n",
    "Load the file `flight_info.csv` into a Spark `DataFrame` called `fight_info`.\n",
    "\n",
    "  * Note that you will need to create a sparkSession prior to loading the data\n",
    "  \n",
    "* How many instances (entries) does the file contain?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ad5e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark import SparkContext\n",
    "#sc = SparkContext()\n",
    "#sc.stop()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "session = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9206e43-adaa-4740-9877-d8659da97657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      " The file contains 450017 instances\n"
     ]
    }
   ],
   "source": [
    "#flight_info = session.read.csv(\"data/flight_info.csv\", header=True)\n",
    "flight_info= session.read.options(inferSchema = True).csv(\"data/flight_info.csv\", header=True)\n",
    "print(type(flight_info))\n",
    "print(f' The file contains {flight_info.count()} instances')\n",
    "#flight_info.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad81f1",
   "metadata": {},
   "source": [
    "### Q20\n",
    "\n",
    "Use `pySpark-SQL` or `pandas-like syntax to compute the airlines represented in this dataset.\n",
    "The airline information is stored in a field called UniqueCarrier, which represents the unique carrier code (ex.AA = American Airlines). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73fd7b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|UniqueCarrier|\n",
      "+-------------+\n",
      "|           NK|\n",
      "|           AA|\n",
      "|           EV|\n",
      "|           B6|\n",
      "|           DL|\n",
      "|           F9|\n",
      "|           HA|\n",
      "|           UA|\n",
      "|           OO|\n",
      "|           WN|\n",
      "|           AS|\n",
      "|           VX|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_info.select('UniqueCarrier').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1349b2f5",
   "metadata": {},
   "source": [
    "### Q21\n",
    "\n",
    "The data file contains various other fields, two of which are useful for answering the next question.\n",
    "\n",
    "* CRSDepTime: Represents the scheduled departure time\n",
    "* DepTime: Represents the actual departure time\n",
    "\n",
    "Compute the number of flights delayed per each carried code represented in this dataset. Sort the data by decreasing order of delays.\n",
    "  * A delay is observed when `DepTime` > `CRSDepTime`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c72cff25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|UniqueCarrier|count|\n",
      "+-------------+-----+\n",
      "|           WN|47472|\n",
      "|           DL|24334|\n",
      "|           AA|23461|\n",
      "|           UA|17701|\n",
      "|           OO|16751|\n",
      "|           EV|11596|\n",
      "|           B6| 9396|\n",
      "|           AS| 4488|\n",
      "|           NK| 4151|\n",
      "|           F9| 2988|\n",
      "|           VX| 2648|\n",
      "|           HA| 1939|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delay_count = flight_info.filter(flight_info['DepTime'] > flight_info['CRSDepTime']).groupBy(flight_info['UniqueCarrier']).count()\n",
    "#print(type(delay_count))\n",
    "#delay_count.show()\n",
    "\n",
    "delay_count.sort('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1645593b",
   "metadata": {},
   "source": [
    "### Q22\n",
    "\n",
    " Use the file `airlines.csv` to find the the complete name of the airline. Here, you are required to load the file as a pyspark DataFrame; call it `airlines_info`, and repeat the query above while including the `flights.csv `file in your query ( requires doing a `join`) so that you can also display the full name of the carrier (second column). \n",
    "\n",
    "The result will look (approximately) like:\n",
    "\n",
    "```\n",
    "[Row(UniqueCarrier='WN', first(_c1)='Southwest Airlines', count=SOME_count),\n",
    " Row(UniqueCarrier='DL', first(_c1)='Delta Air Lines', count=SOME_count),\n",
    " Row(UniqueCarrier='AA', first(_c1)='American Airlines', count=SOME_count),\n",
    " ...\n",
    " ]\n",
    "```\n",
    "\n",
    "The carrier code in the `airlines.csv` file is provided in the 4th (1-based) column\n",
    "\n",
    "Note that the file `airlines.csv` does not have column header. Hence, you need to print one line of your dataset to see what names Spark gave to the columns. Use the name provided by Spark in your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30cea22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0=1, _c1='Private flight', _c2='\\\\N', _c3='-', _c4='N/A', _c5=None, _c6=None, _c7='Y')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_info= session.read.options(inferSchema = True).csv(\"data/airlines.csv\", header=False)\n",
    "airline_info.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7cd8dafa-8d25-4d9c-81ec-8bc53d9d26f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0=0, DayOfWeek=2, UniqueCarrier='AA', FlightNum=494, Origin='CLT', Dest='PHX', CRSDepTime=1619, DepTime=1616.0, TaxiOut=17.0, WheelsOff=1633.0, WheelsOn=1837.0, TaxiIn=5.0, CRSArrTime=1856, ArrTime=1842.0, Cancelled=0.0, CancellationCode=None, Distance=1773.0, CarrierDelay=None, WeatherDelay=None, NASDelay=None, SecurityDelay=None, LateAircraftDelay=None, _c0=24, _c1='American Airlines', _c2='\\\\N', _c3='AA', _c4='AAL', _c5='AMERICAN', _c6='United States', _c7='Y')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_data = flight_info.join(airline_info,delay_count['UniqueCarrier']== airline_info['_c3'])\n",
    "joined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7e0b2d8-1b94-48dd-a250-e29c4bbdd2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(UniqueCarrier='B6', _c1='JetBlue Airways', count=9396)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#delay_count = flight_info.filter(flight_info['DepTime'] > flight_info['CRSDepTime']).groupBy(flight_info['UniqueCarrier']).count()\n",
    "group_cols=['UniqueCarrier','_c1']\n",
    "new_delay_count = joined_data.filter(joined_data['DepTime'] > joined_data['CRSDepTime']).groupBy(group_cols).count()\n",
    "new_delay_count.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "213c92e8-4569-4f9a-8574-0a7409eda637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----+\n",
      "|UniqueCarrier|                 _c1|count|\n",
      "+-------------+--------------------+-----+\n",
      "|           WN|  Southwest Airlines|47472|\n",
      "|           DL|     Delta Air Lines|24334|\n",
      "|           AA|   American Airlines|23461|\n",
      "|           UA|     United Airlines|17701|\n",
      "|           OO|             SkyWest|16751|\n",
      "|           EV|Atlantic Southeas...|11596|\n",
      "|           B6|     JetBlue Airways| 9396|\n",
      "|           AS|     Alaska Airlines| 4488|\n",
      "|           NK|     Spirit Airlines| 4151|\n",
      "|           F9|   Frontier Airlines| 2988|\n",
      "|           VX|      Virgin America| 2648|\n",
      "|           HA|   Hawaiian Airlines| 1939|\n",
      "+-------------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_delay_count.sort('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0123a8",
   "metadata": {},
   "source": [
    "### Q23\n",
    "\n",
    "Compute the number of delays per company per day. The day is encoded as an integer in the column `DayOfWeek` in `fight_info`. You can display the day as an integer or map it into a string name of the week.\n",
    "Sort the data by airline code (UniqueCarrier) and by increasing values of DayOfWeek\n",
    "\n",
    "\n",
    "You results should look like the following\n",
    "\n",
    "[Row(UniqueCarrier='WN', DayOfWeek='6', COUNT=5270), Row(UniqueCarrier='WN', DayOfWeek='7', COUNT=8681) ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9971baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(UniqueCarrier='F9', DayOfWeek=2, count=469)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_col = ['UniqueCarrier','DayOfWeek']\n",
    "delay_by_day = joined_data.filter(joined_data['DepTime'] > joined_data['CRSDepTime']).groupBy(group_col).count()\n",
    "delay_by_day.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc9e7453-1b10-4e29-8465-06ad705017be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-----+\n",
      "|UniqueCarrier|DayOfWeek|count|\n",
      "+-------------+---------+-----+\n",
      "|           AA|        1| 4639|\n",
      "|           AA|        2| 3288|\n",
      "|           AA|        3| 2570|\n",
      "|           AA|        4| 3142|\n",
      "|           AA|        5| 3108|\n",
      "|           AA|        6| 2509|\n",
      "|           AA|        7| 4205|\n",
      "|           AS|        1|  786|\n",
      "|           AS|        2|  580|\n",
      "|           AS|        3|  573|\n",
      "|           AS|        4|  656|\n",
      "|           AS|        5|  554|\n",
      "|           AS|        6|  470|\n",
      "|           AS|        7|  869|\n",
      "|           B6|        1| 1754|\n",
      "|           B6|        2| 1628|\n",
      "|           B6|        3| 1109|\n",
      "|           B6|        4| 1101|\n",
      "|           B6|        5| 1175|\n",
      "|           B6|        6|  900|\n",
      "+-------------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sort by airline code THEN (ascending) day of week\n",
    "delay_by_day.sort(delay_by_day.UniqueCarrier.asc(),delay_by_day.DayOfWeek.asc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c955760",
   "metadata": {},
   "source": [
    "### Q24  \n",
    "\n",
    "Counting the number of delayed flights per airline is misleading, as airlines with more flights are more likley to have delays than companies with substantially fiewer flights. \n",
    "\n",
    "Repeat the same query above but, for each carrier, normalize the counts of delays by the total number of flights for that carrier. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13ae9b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|UniqueCarrier|count|UniqueCarrier|total_count|   normalized_delay|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|           NK| 4151|           NK|      12570|  0.330230708035004|\n",
      "|           AA|23461|           AA|      73132|0.32080347864136083|\n",
      "|           EV|11596|           EV|      35037| 0.3309644090532865|\n",
      "|           B6| 9396|           B6|      24602|0.38192016909194376|\n",
      "|           DL|24334|           DL|      69813|0.34855972383366995|\n",
      "|           F9| 2988|           F9|       7760| 0.3850515463917526|\n",
      "|           HA| 1939|           HA|       6276|0.30895474824729124|\n",
      "|           UA|17701|           UA|      42403| 0.4174468787585784|\n",
      "|           OO|16751|           OO|      50146|0.33404458979779045|\n",
      "|           WN|47472|           WN|     107785| 0.4404323421626386|\n",
      "|           AS| 4488|           AS|      14711|0.30507783291414586|\n",
      "|           VX| 2648|           VX|       5782| 0.4579730197163611|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|UniqueCarrier|count|UniqueCarrier|total_count|   normalized_delay|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|           NK| 4151|           NK|      12570|  0.330230708035004|\n",
      "|           AA|23461|           AA|      73132|0.32080347864136083|\n",
      "|           EV|11596|           EV|      35037| 0.3309644090532865|\n",
      "|           B6| 9396|           B6|      24602|0.38192016909194376|\n",
      "|           DL|24334|           DL|      69813|0.34855972383366995|\n",
      "|           F9| 2988|           F9|       7760| 0.3850515463917526|\n",
      "|           HA| 1939|           HA|       6276|0.30895474824729124|\n",
      "|           UA|17701|           UA|      42403| 0.4174468787585784|\n",
      "|           OO|16751|           OO|      50146|0.33404458979779045|\n",
      "|           WN|47472|           WN|     107785| 0.4404323421626386|\n",
      "|           AS| 4488|           AS|      14711|0.30507783291414586|\n",
      "|           VX| 2648|           VX|       5782| 0.4579730197163611|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|UniqueCarrier|count|UniqueCarrier|total_count|   normalized_delay|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|           NK| 4151|           NK|      12570|  0.330230708035004|\n",
      "|           AA|23461|           AA|      73132|0.32080347864136083|\n",
      "|           EV|11596|           EV|      35037| 0.3309644090532865|\n",
      "|           B6| 9396|           B6|      24602|0.38192016909194376|\n",
      "|           DL|24334|           DL|      69813|0.34855972383366995|\n",
      "|           F9| 2988|           F9|       7760| 0.3850515463917526|\n",
      "|           HA| 1939|           HA|       6276|0.30895474824729124|\n",
      "|           UA|17701|           UA|      42403| 0.4174468787585784|\n",
      "|           OO|16751|           OO|      50146|0.33404458979779045|\n",
      "|           WN|47472|           WN|     107785| 0.4404323421626386|\n",
      "|           AS| 4488|           AS|      14711|0.30507783291414586|\n",
      "|           VX| 2648|           VX|       5782| 0.4579730197163611|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|UniqueCarrier|count|UniqueCarrier|total_count|   normalized_delay|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|           NK| 4151|           NK|      12570|  0.330230708035004|\n",
      "|           AA|23461|           AA|      73132|0.32080347864136083|\n",
      "|           EV|11596|           EV|      35037| 0.3309644090532865|\n",
      "|           B6| 9396|           B6|      24602|0.38192016909194376|\n",
      "|           DL|24334|           DL|      69813|0.34855972383366995|\n",
      "|           F9| 2988|           F9|       7760| 0.3850515463917526|\n",
      "|           HA| 1939|           HA|       6276|0.30895474824729124|\n",
      "|           UA|17701|           UA|      42403| 0.4174468787585784|\n",
      "|           OO|16751|           OO|      50146|0.33404458979779045|\n",
      "|           WN|47472|           WN|     107785| 0.4404323421626386|\n",
      "|           AS| 4488|           AS|      14711|0.30507783291414586|\n",
      "|           VX| 2648|           VX|       5782| 0.4579730197163611|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|UniqueCarrier|count|UniqueCarrier|total_count|   normalized_delay|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|           NK| 4151|           NK|      12570|  0.330230708035004|\n",
      "|           AA|23461|           AA|      73132|0.32080347864136083|\n",
      "|           EV|11596|           EV|      35037| 0.3309644090532865|\n",
      "|           B6| 9396|           B6|      24602|0.38192016909194376|\n",
      "|           DL|24334|           DL|      69813|0.34855972383366995|\n",
      "|           F9| 2988|           F9|       7760| 0.3850515463917526|\n",
      "|           HA| 1939|           HA|       6276|0.30895474824729124|\n",
      "|           UA|17701|           UA|      42403| 0.4174468787585784|\n",
      "|           OO|16751|           OO|      50146|0.33404458979779045|\n",
      "|           WN|47472|           WN|     107785| 0.4404323421626386|\n",
      "|           AS| 4488|           AS|      14711|0.30507783291414586|\n",
      "|           VX| 2648|           VX|       5782| 0.4579730197163611|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|UniqueCarrier|count|UniqueCarrier|total_count|   normalized_delay|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|           NK| 4151|           NK|      12570|  0.330230708035004|\n",
      "|           AA|23461|           AA|      73132|0.32080347864136083|\n",
      "|           EV|11596|           EV|      35037| 0.3309644090532865|\n",
      "|           B6| 9396|           B6|      24602|0.38192016909194376|\n",
      "|           DL|24334|           DL|      69813|0.34855972383366995|\n",
      "|           F9| 2988|           F9|       7760| 0.3850515463917526|\n",
      "|           HA| 1939|           HA|       6276|0.30895474824729124|\n",
      "|           UA|17701|           UA|      42403| 0.4174468787585784|\n",
      "|           OO|16751|           OO|      50146|0.33404458979779045|\n",
      "|           WN|47472|           WN|     107785| 0.4404323421626386|\n",
      "|           AS| 4488|           AS|      14711|0.30507783291414586|\n",
      "|           VX| 2648|           VX|       5782| 0.4579730197163611|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|UniqueCarrier|count|UniqueCarrier|total_count|   normalized_delay|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|           NK| 4151|           NK|      12570|  0.330230708035004|\n",
      "|           AA|23461|           AA|      73132|0.32080347864136083|\n",
      "|           EV|11596|           EV|      35037| 0.3309644090532865|\n",
      "|           B6| 9396|           B6|      24602|0.38192016909194376|\n",
      "|           DL|24334|           DL|      69813|0.34855972383366995|\n",
      "|           F9| 2988|           F9|       7760| 0.3850515463917526|\n",
      "|           HA| 1939|           HA|       6276|0.30895474824729124|\n",
      "|           UA|17701|           UA|      42403| 0.4174468787585784|\n",
      "|           OO|16751|           OO|      50146|0.33404458979779045|\n",
      "|           WN|47472|           WN|     107785| 0.4404323421626386|\n",
      "|           AS| 4488|           AS|      14711|0.30507783291414586|\n",
      "|           VX| 2648|           VX|       5782| 0.4579730197163611|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|UniqueCarrier|count|UniqueCarrier|total_count|   normalized_delay|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|           NK| 4151|           NK|      12570|  0.330230708035004|\n",
      "|           AA|23461|           AA|      73132|0.32080347864136083|\n",
      "|           EV|11596|           EV|      35037| 0.3309644090532865|\n",
      "|           B6| 9396|           B6|      24602|0.38192016909194376|\n",
      "|           DL|24334|           DL|      69813|0.34855972383366995|\n",
      "|           F9| 2988|           F9|       7760| 0.3850515463917526|\n",
      "|           HA| 1939|           HA|       6276|0.30895474824729124|\n",
      "|           UA|17701|           UA|      42403| 0.4174468787585784|\n",
      "|           OO|16751|           OO|      50146|0.33404458979779045|\n",
      "|           WN|47472|           WN|     107785| 0.4404323421626386|\n",
      "|           AS| 4488|           AS|      14711|0.30507783291414586|\n",
      "|           VX| 2648|           VX|       5782| 0.4579730197163611|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "\n",
      "1.12 s ± 117 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "total_flights = flight_info.groupBy('UniqueCarrier').count()\n",
    "#total_flights.show()\n",
    "\n",
    "#rename count column\n",
    "total_flights_new = total_flights.withColumnRenamed('count','total_count')\n",
    "#total_flights_new.show()\n",
    "\n",
    "new_df= delay_count.join(total_flights_new, delay_count['UniqueCarrier']==total_flights_new['UniqueCarrier'])\n",
    "new_df.withColumn('normalized_delay', new_df['count']/new_df['total_count']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b73b3e4",
   "metadata": {},
   "source": [
    "### Q25 \n",
    "\n",
    "Time the query above. How long did it take to run. \n",
    "  * Make sure you run the code a few times and compute the average run time.\n",
    "  * The above should be easy to implement if you use the correct Jupyter Notebook `magic` function\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6190ee-efc2-434e-9fc6-461fb9f8390c",
   "metadata": {},
   "source": [
    "### %%timeit Results\n",
    "1.05 s ± 62.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "**Please note results may differ if code is reloaded. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604e85e0",
   "metadata": {},
   "source": [
    "### Q26 \n",
    "\n",
    "Use one of the techniques covered in class to accelerate this query. Time your query to see by how much the run time was improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e951d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|UniqueCarrier|count|UniqueCarrier|total_count|   normalized_delay|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|           UA|17701|           UA|      42403| 0.4174468787585784|\n",
      "|           NK| 4151|           NK|      12570|  0.330230708035004|\n",
      "|           AA|23461|           AA|      73132|0.32080347864136083|\n",
      "|           EV|11596|           EV|      35037| 0.3309644090532865|\n",
      "|           B6| 9396|           B6|      24602|0.38192016909194376|\n",
      "|           DL|24334|           DL|      69813|0.34855972383366995|\n",
      "|           OO|16751|           OO|      50146|0.33404458979779045|\n",
      "|           F9| 2988|           F9|       7760| 0.3850515463917526|\n",
      "|           HA| 1939|           HA|       6276|0.30895474824729124|\n",
      "|           AS| 4488|           AS|      14711|0.30507783291414586|\n",
      "|           VX| 2648|           VX|       5782| 0.4579730197163611|\n",
      "|           WN|47472|           WN|     107785| 0.4404323421626386|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|UniqueCarrier|count|UniqueCarrier|total_count|   normalized_delay|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|           UA|17701|           UA|      42403| 0.4174468787585784|\n",
      "|           NK| 4151|           NK|      12570|  0.330230708035004|\n",
      "|           AA|23461|           AA|      73132|0.32080347864136083|\n",
      "|           EV|11596|           EV|      35037| 0.3309644090532865|\n",
      "|           B6| 9396|           B6|      24602|0.38192016909194376|\n",
      "|           DL|24334|           DL|      69813|0.34855972383366995|\n",
      "|           OO|16751|           OO|      50146|0.33404458979779045|\n",
      "|           F9| 2988|           F9|       7760| 0.3850515463917526|\n",
      "|           HA| 1939|           HA|       6276|0.30895474824729124|\n",
      "|           AS| 4488|           AS|      14711|0.30507783291414586|\n",
      "|           VX| 2648|           VX|       5782| 0.4579730197163611|\n",
      "|           WN|47472|           WN|     107785| 0.4404323421626386|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|UniqueCarrier|count|UniqueCarrier|total_count|   normalized_delay|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|           UA|17701|           UA|      42403| 0.4174468787585784|\n",
      "|           NK| 4151|           NK|      12570|  0.330230708035004|\n",
      "|           AA|23461|           AA|      73132|0.32080347864136083|\n",
      "|           EV|11596|           EV|      35037| 0.3309644090532865|\n",
      "|           B6| 9396|           B6|      24602|0.38192016909194376|\n",
      "|           DL|24334|           DL|      69813|0.34855972383366995|\n",
      "|           OO|16751|           OO|      50146|0.33404458979779045|\n",
      "|           F9| 2988|           F9|       7760| 0.3850515463917526|\n",
      "|           HA| 1939|           HA|       6276|0.30895474824729124|\n",
      "|           AS| 4488|           AS|      14711|0.30507783291414586|\n",
      "|           VX| 2648|           VX|       5782| 0.4579730197163611|\n",
      "|           WN|47472|           WN|     107785| 0.4404323421626386|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|UniqueCarrier|count|UniqueCarrier|total_count|   normalized_delay|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|           UA|17701|           UA|      42403| 0.4174468787585784|\n",
      "|           NK| 4151|           NK|      12570|  0.330230708035004|\n",
      "|           AA|23461|           AA|      73132|0.32080347864136083|\n",
      "|           EV|11596|           EV|      35037| 0.3309644090532865|\n",
      "|           B6| 9396|           B6|      24602|0.38192016909194376|\n",
      "|           DL|24334|           DL|      69813|0.34855972383366995|\n",
      "|           OO|16751|           OO|      50146|0.33404458979779045|\n",
      "|           F9| 2988|           F9|       7760| 0.3850515463917526|\n",
      "|           HA| 1939|           HA|       6276|0.30895474824729124|\n",
      "|           AS| 4488|           AS|      14711|0.30507783291414586|\n",
      "|           VX| 2648|           VX|       5782| 0.4579730197163611|\n",
      "|           WN|47472|           WN|     107785| 0.4404323421626386|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|UniqueCarrier|count|UniqueCarrier|total_count|   normalized_delay|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|           UA|17701|           UA|      42403| 0.4174468787585784|\n",
      "|           NK| 4151|           NK|      12570|  0.330230708035004|\n",
      "|           AA|23461|           AA|      73132|0.32080347864136083|\n",
      "|           EV|11596|           EV|      35037| 0.3309644090532865|\n",
      "|           B6| 9396|           B6|      24602|0.38192016909194376|\n",
      "|           DL|24334|           DL|      69813|0.34855972383366995|\n",
      "|           OO|16751|           OO|      50146|0.33404458979779045|\n",
      "|           F9| 2988|           F9|       7760| 0.3850515463917526|\n",
      "|           HA| 1939|           HA|       6276|0.30895474824729124|\n",
      "|           AS| 4488|           AS|      14711|0.30507783291414586|\n",
      "|           VX| 2648|           VX|       5782| 0.4579730197163611|\n",
      "|           WN|47472|           WN|     107785| 0.4404323421626386|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|UniqueCarrier|count|UniqueCarrier|total_count|   normalized_delay|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|           UA|17701|           UA|      42403| 0.4174468787585784|\n",
      "|           NK| 4151|           NK|      12570|  0.330230708035004|\n",
      "|           AA|23461|           AA|      73132|0.32080347864136083|\n",
      "|           EV|11596|           EV|      35037| 0.3309644090532865|\n",
      "|           B6| 9396|           B6|      24602|0.38192016909194376|\n",
      "|           DL|24334|           DL|      69813|0.34855972383366995|\n",
      "|           OO|16751|           OO|      50146|0.33404458979779045|\n",
      "|           F9| 2988|           F9|       7760| 0.3850515463917526|\n",
      "|           HA| 1939|           HA|       6276|0.30895474824729124|\n",
      "|           AS| 4488|           AS|      14711|0.30507783291414586|\n",
      "|           VX| 2648|           VX|       5782| 0.4579730197163611|\n",
      "|           WN|47472|           WN|     107785| 0.4404323421626386|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|UniqueCarrier|count|UniqueCarrier|total_count|   normalized_delay|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|           UA|17701|           UA|      42403| 0.4174468787585784|\n",
      "|           NK| 4151|           NK|      12570|  0.330230708035004|\n",
      "|           AA|23461|           AA|      73132|0.32080347864136083|\n",
      "|           EV|11596|           EV|      35037| 0.3309644090532865|\n",
      "|           B6| 9396|           B6|      24602|0.38192016909194376|\n",
      "|           DL|24334|           DL|      69813|0.34855972383366995|\n",
      "|           OO|16751|           OO|      50146|0.33404458979779045|\n",
      "|           F9| 2988|           F9|       7760| 0.3850515463917526|\n",
      "|           HA| 1939|           HA|       6276|0.30895474824729124|\n",
      "|           AS| 4488|           AS|      14711|0.30507783291414586|\n",
      "|           VX| 2648|           VX|       5782| 0.4579730197163611|\n",
      "|           WN|47472|           WN|     107785| 0.4404323421626386|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|UniqueCarrier|count|UniqueCarrier|total_count|   normalized_delay|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "|           UA|17701|           UA|      42403| 0.4174468787585784|\n",
      "|           NK| 4151|           NK|      12570|  0.330230708035004|\n",
      "|           AA|23461|           AA|      73132|0.32080347864136083|\n",
      "|           EV|11596|           EV|      35037| 0.3309644090532865|\n",
      "|           B6| 9396|           B6|      24602|0.38192016909194376|\n",
      "|           DL|24334|           DL|      69813|0.34855972383366995|\n",
      "|           OO|16751|           OO|      50146|0.33404458979779045|\n",
      "|           F9| 2988|           F9|       7760| 0.3850515463917526|\n",
      "|           HA| 1939|           HA|       6276|0.30895474824729124|\n",
      "|           AS| 4488|           AS|      14711|0.30507783291414586|\n",
      "|           VX| 2648|           VX|       5782| 0.4579730197163611|\n",
      "|           WN|47472|           WN|     107785| 0.4404323421626386|\n",
      "+-------------+-----+-------------+-----------+-------------------+\n",
      "\n",
      "731 ms ± 60.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "##Caching Memory Approach\n",
    "# spark.catalog.dataFrame.cache()\n",
    "\n",
    "delay_count_c = flight_info.filter(flight_info['DepTime'] > flight_info['CRSDepTime']).groupBy(flight_info['UniqueCarrier']).count().cache()\n",
    "#print(type(delay_count))\n",
    "\n",
    "total_flights_c = flight_info.groupBy('UniqueCarrier').count().cache()\n",
    "\n",
    "\n",
    "#rename count column\n",
    "total_flights_new_c = total_flights_c.withColumnRenamed('count','total_count').cache()\n",
    "\n",
    "#total_flights_new.show()\n",
    "\n",
    "new_df_c= delay_count.join(total_flights_new_c, delay_count_c['UniqueCarrier']==total_flights_new_c['UniqueCarrier'])\n",
    "new_df_c.withColumn('normalized_delay', new_df_c['count']/new_df_c['total_count']).show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d74ae9-43bf-4b10-9125-f7451ce3447e",
   "metadata": {},
   "source": [
    "#### %%timeit result post cache optimization:\n",
    "731 ms ± 60.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a87a433",
   "metadata": {},
   "source": [
    "### Q27 \n",
    "\n",
    "Is the departure delay (i.e., DepTime - CRSDepTime) predictive of the arrival delay (ArrTime > CRSArrTime)?\n",
    "Use an approach of your choice (e.g. `skelearn` which we covered in class or `Spark`) to model as a linear regression the arrival delay as a function of the departure delay. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8dbd52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0=3, DayOfWeek=5, UniqueCarrier='AA', FlightNum=494, Origin='CLT', Dest='PHX', CRSDepTime=1619, DepTime=1656.0, TaxiOut=18.0, WheelsOff=1714.0, WheelsOn=1926.0, TaxiIn=3.0, CRSArrTime=1856, ArrTime=1929.0, Cancelled=0.0, CancellationCode=None, Distance=1773.0, CarrierDelay=33.0, WeatherDelay=0.0, NASDelay=0.0, SecurityDelay=0.0, LateAircraftDelay=0.0, dep_delay_calc=37.0, arr_delay_calc=73.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "##Approach\n",
    "#1. Calculate departure_delay --> feature\n",
    "#2. Calculate arrival_delay --> target\n",
    "#3. Model Regression\n",
    "#4. Plot Regression\n",
    "\n",
    "flight_delays = flight_info.filter(flight_info['DepTime'] > flight_info['CRSDepTime'])\n",
    "flight_delay_calc =flight_delays.withColumn('dep_delay_calc',flight_delays['DepTime']- flight_delays['CRSDepTime']).withColumn('arr_delay_calc',flight_delays['ArrTime']- flight_delays['CRSArrTime'])\n",
    "flight_delay_calc.head()\n",
    "\n",
    "\n",
    "#https://inria.github.io/scikit-learn-mooc/python_scripts/linear_regression_in_sklearn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0de7574-9e9a-46e2-9183-92d8cd5e26c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [48], line 23\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m##FIX Attempts##\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#print(type(data))\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#print((data.count(), len(data.columns))) #shape\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#data_arr = np.array(data.collect()).reshape(-1,1)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#print(type(data_arr))\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#data_arr.shape()\u001b[39;00m\n\u001b[1;32m     22\u001b[0m linear_regression \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[0;32m---> 23\u001b[0m \u001b[43mlinear_regression\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_array\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_base.py:736\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m outs])\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, _, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingular_ \u001b[38;5;241m=\u001b[39m \u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstsq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/linalg/_basic.py:1135\u001b[0m, in \u001b[0;36mlstsq\u001b[0;34m(a, b, cond, overwrite_a, overwrite_b, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03mCompute least-squares solution to equation Ax = b.\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \n\u001b[1;32m   1133\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m a1 \u001b[38;5;241m=\u001b[39m _asarray_validated(a, check_finite\u001b[38;5;241m=\u001b[39mcheck_finite)\n\u001b[0;32m-> 1135\u001b[0m b1 \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_validated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_finite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(a1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput array a should be 2D\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/_lib/_util.py:287\u001b[0m, in \u001b[0;36m_asarray_validated\u001b[0;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasked arrays are not supported\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    286\u001b[0m toarray \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray_chkfinite \u001b[38;5;28;01mif\u001b[39;00m check_finite \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray\n\u001b[0;32m--> 287\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m objects_ok:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/function_base.py:603\u001b[0m, in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    601\u001b[0m a \u001b[38;5;241m=\u001b[39m asarray(a, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mchar \u001b[38;5;129;01min\u001b[39;00m typecodes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllFloat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(a)\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m--> 603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray must not contain infs or NaNs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "feature_name='dep_delay_calc' #independent var\n",
    "target_name='arr_delay_calc'  #depdendent var\n",
    "#target = flight_delay_calc[target_name].collect()\n",
    "\n",
    "\n",
    "#Dataframe to array\n",
    "data_array=np.array(flight_delay_calc.select(feature_name).collect())\n",
    "target_array =np.array( flight_delay_calc.select(target_name).collect())\n",
    "print(type(data_array))\n",
    "data_array.reshape(-1,1) #2D array\n",
    "\n",
    "\n",
    "\n",
    "##FIX Attempts##\n",
    "#print(type(data))\n",
    "#print((data.count(), len(data.columns))) #shape\n",
    "#data_arr = np.array(data.collect()).reshape(-1,1)\n",
    "#print(type(data_arr))\n",
    "#data_arr.shape()\n",
    "\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(data_array,target_array)  #<-- error \n",
    "\n",
    "#ValueError: Expected 2D array, got scalar array instead:\n",
    "#array=DataFrame[dep_delay_calc: double].\n",
    "#Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n",
    "\n",
    "#New Error\n",
    "#ValueError: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.\n",
    "\n",
    "\n",
    "#weighted_dep_delay = linear_regression.coef_[0]\n",
    "#weighted_dep_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21d085-900c-4106-a520-6c72e46024c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
